{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK - PERFORMING RANDOM FOREST CLASSIFICATION ALGORITHM ON IRIS DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest : \n",
    "- It's a supervised learning algorithm which is used for both classification as well as regression. \n",
    "\n",
    "- Random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is Random Forest an ensemble model?\n",
    "\n",
    "- Random forest is an ensemble machine learning algorithm. \n",
    "- It is perhaps the most popular and widely used machine learning algorithm given its good or excellent performance across a wide range of classification and regression predictive modeling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Ensemble Algorithm: \n",
    "\n",
    "- Ensemble is a proven method for improving the accuracy of the model and works in most of the cases. \n",
    "- Ensemble makes the model more robust and stable thus ensuring decent performance on the test cases in most scenarios. \n",
    "- We can use ensemble to capture linear and simple as well nonlinear complex relationships in the data. \n",
    "- This can be done by using two different models and forming an ensemble of two. \n",
    "\n",
    "\n",
    "#### Disadvantages of Ensemble Algorithm: \n",
    "\n",
    "- Ensemble reduces the model interpret-ability and makes it very difficult to draw any crucial business insights at the end. \n",
    "- It is time-consuming and thus might not be the best idea for real-time applications. \n",
    "- The selection of models for creating an ensemble is an art which is really hard to master. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the Random forest algorithm work?\n",
    "\n",
    "#### It works in four steps:\n",
    "\n",
    "1. Select the prediction result with the most votes as the final prediction.Select random samples from a given dataset.\n",
    "\n",
    "2. Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
    "\n",
    "3. Perform a vote for each predicted result.\n",
    "\n",
    "4. Select the prediction result with the most votes as the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest Classifier using Scikit-learn\n",
    "\n",
    "#### We will be building a model on the iris flower dataset, which is a very famous classification set. \n",
    "\n",
    "1. It comprises the sepal length, sepal width, petal length, petal width, and type of flowers. \n",
    "\n",
    "2. There are three species or classes: setosa, versicolor, and virginia. You will build a model to classify the type of flower. \n",
    "\n",
    "3. The dataset is available in the scikit-learn library or you can download it from the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is random forest better than decision tree?\n",
    "\n",
    "- But as stated, a random forest is a collection of decision trees. \n",
    "- With that, random forests are a strong modeling technique and much more robust than a single decision tree. \n",
    "- They aggregate many decision trees to limit overfitting as well as error due to bias and therefore yield useful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by importing the datasets library from scikit-learn, and load the iris dataset with load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can print the target and feature names, to make sure you have the right dataset, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# print the label species(setosa, versicolor,virginica)\n",
    "print(iris.target_names)\n",
    "\n",
    "# print the names of the four features\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's a good idea to always explore your data a bit, so you know what you're working with. Here, you can see the first five rows of the dataset are printed, as well as the target variable for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the iris data (top 5 records)\n",
    "print(iris.data[0:5])\n",
    "\n",
    "# print the iris labels (0:setosa, 1:versicolor, 2:virginica)\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, you can create a DataFrame of the iris dataset the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame of given iris dataset.\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({'sepal length':iris.data[:,0], 'sepal width':iris.data[:,1], 'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3], 'species':iris.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width  species\n",
       "0             5.1          3.5           1.4          0.2        0\n",
       "1             4.9          3.0           1.4          0.2        0\n",
       "2             4.7          3.2           1.3          0.2        0\n",
       "3             4.6          3.1           1.5          0.2        0\n",
       "4             5.0          3.6           1.4          0.2        0\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3        2\n",
       "146           6.3          2.5           5.0          1.9        2\n",
       "147           6.5          3.0           5.2          2.0        2\n",
       "148           6.2          3.4           5.4          2.3        2\n",
       "149           5.9          3.0           5.1          1.8        2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal length  150 non-null    float64\n",
      " 1   sepal width   150 non-null    float64\n",
      " 2   petal length  150 non-null    float64\n",
      " 3   petal width   150 non-null    float64\n",
      " 4   species       150 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 5.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length  sepal width  petal length  petal width     species\n",
       "count    150.000000   150.000000    150.000000   150.000000  150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333    1.000000\n",
       "std        0.828066     0.435866      1.765298     0.762238    0.819232\n",
       "min        4.300000     2.000000      1.000000     0.100000    0.000000\n",
       "25%        5.100000     2.800000      1.600000     0.300000    0.000000\n",
       "50%        5.800000     3.000000      4.350000     1.300000    1.000000\n",
       "75%        6.400000     3.300000      5.100000     1.800000    2.000000\n",
       "max        7.900000     4.400000      6.900000     2.500000    2.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, you separate the columns into dependent and independent variables (or features and labels). Then you split those variables into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "y = data['species']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After splitting, you will train the model on the training set and perform predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training, check the accuracy using actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also make a prediction for a single item, for example:\n",
    "\n",
    "#### sepal length = 3\n",
    "#### sepal width = 5\n",
    "#### petal length = 4\n",
    "#### petal width = 2\n",
    "\n",
    "### Now you can predict which type of flower it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[3, 5, 4, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, 2 indicates the flower type Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Important Features in Scikit-learn\n",
    "\n",
    "#### Here, you are finding important features or selecting features in the IRIS dataset. In scikit-learn, you can perform this task in the following steps:\n",
    "\n",
    "- First, you need to create a random forests model.\n",
    "- Second, use the feature importance variable to see feature importance scores.\n",
    "- Third, visualize these scores using the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "petal length (cm)    0.439209\n",
       "petal width (cm)     0.434922\n",
       "sepal length (cm)    0.094094\n",
       "sepal width (cm)     0.031774\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "feature_imp = pd.Series(clf.feature_importances_, index = iris.feature_names).sort_values(ascending = False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also visualize the feature importance. Visualizations are easy to understand and interpretable.\n",
    "\n",
    "#### For visualization, you can use a combination of matplotlib and seaborn.\n",
    "- Because seaborn is built on top of matplotlib, it offers a number of customized themes and provides additional plot types. \n",
    "- Matplotlib is a superset of seaborn and both are equally important for good visualizations.\n",
    "\n",
    "#### %matplotlib: %matplotlib inline sets the backend of matplotlib to the 'inline' backend: \n",
    "- With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it. \n",
    "- The resulting plots will then also be stored in the notebook document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a bar plot\n",
    "sns.barplot(x = feature_imp, y = feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "\n",
    "plt.ylabel('Features')\n",
    "\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Model on Selected Features\n",
    "- Here, you can remove the \"sepal width\" feature because it has very low importance, and select the 3 remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6dd7d6c051dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import train_test_split function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Split dataset into features and labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'petal length'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'petal width'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sepal length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Removed feature \"sepal length\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split dataset into features and labels\n",
    "X = data[['petal length', 'petal width','sepal length']]  # Removed feature \"sepal length\"\n",
    "y = data['species']                                       \n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.70, random_state = 5) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After spliting, you will generate a model on the selected training set features, perform predictions on the selected test set features, and compare actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# prediction on test set\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that after removing the least important features (sepal length), the accuracy increased. This is because you removed misleading data and noise, resulting in an increased accuracy. \n",
    "\n",
    "#### A lesser amount of features also reduces the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Random Forest:\n",
    "\n",
    "1. Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n",
    "\n",
    "2. It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n",
    "\n",
    "3. The algorithm can be used in both classification and regression problems.\n",
    "\n",
    "4. Random forests can also handle missing values. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n",
    "\n",
    "5. You can get the relative feature importance, which helps in selecting the most contributing features for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of Random Forest:\n",
    "\n",
    "1. Random forests is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
    "\n",
    "2. The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests vs Decision Trees\n",
    "\n",
    "1. Random forests is a set of multiple decision trees.\n",
    "\n",
    "2. Deep decision trees may suffer from overfitting, but random forests prevents overfitting by creating trees on random subsets.\n",
    "\n",
    "3. Decision trees are computationally faster.\n",
    "\n",
    "4. Random forests is difficult to interpret, while a decision tree is easily interpretable and can be converted to rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
